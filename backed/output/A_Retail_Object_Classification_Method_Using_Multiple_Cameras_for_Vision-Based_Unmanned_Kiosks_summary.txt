This research proposes a novel view-aware object classification method designed to improve recognition accuracy in vision-based unmanned kiosks, specifically targeting the challenges posed by limited detections and high interclass similarity often encountered with retail products.  The core of the innovation lies in a view-based annotation scheme, where each object is categorized into subclasses based on different camera viewpoints (top, middle, and bottom), providing a more granular representation of the object's appearance.  This scheme allows for the training of Convolutional Neural Networks (CNNs), particularly variants of YOLO, on these specific subclasses, generating view-specific feature vectors composed of subclass probabilities.  These vectors, derived from multiple camera feeds, are then aggregated to create a comprehensive view-aware feature, capturing the object's appearance from various perspectives.  Unlike traditional methods relying on majority voting from a single, conventionally trained detector, this approach utilizes a Random Forest (RF) classifier for final object superclass (original category) prediction.  The choice of RF stems from its robustness in handling missing values, which can occur when certain viewpoints are obstructed, and its ability to discern the importance of different viewpoints for accurate classification.  Experimental validation using a prototype kiosk system and a custom-built retail product dataset, publicly available on IEEE DataPort for reproducibility, demonstrates a substantial performance improvement compared to conventional methods.  This superior performance is attributed to the view-based annotation strategy, which effectively reduces false positives by minimizing intraclass variance, and the RF classifier's capability to identify and leverage informative viewpoints to resolve ambiguities arising from interclass similarity.  The research highlights the effectiveness of incorporating multiple viewpoints and a sophisticated classifier for enhanced object recognition in challenging retail environments.  Furthermore, the availability of the dataset encourages future research and development in this area.  Potential avenues for improvement include incorporating temporal information through object tracking, thereby enhancing the system's ability to handle dynamic scenes and occlusions.  Additionally, integrating attention-based mechanisms could further refine the model by focusing on the most discriminative viewpoints, optimizing resource allocation, and potentially improving both accuracy and efficiency.  By capturing the object's appearance from multiple angles and employing a robust classifier, this view-aware approach provides a significant step towards more reliable and accurate object recognition in unmanned kiosk systems.  The rigorous evaluation and the publicly available dataset pave the way for further exploration and refinement of this promising technique. This work offers a valuable contribution to the field of computer vision, specifically addressing the unique challenges present in real-world retail applications.  The focus on view-based features and the utilization of a robust classifier demonstrates a practical and effective strategy for improving object recognition in complex scenarios. The meticulous design of the experiments and the transparent sharing of the dataset solidifies the research's contribution to the advancement of automated retail technologies.  Finally, the identified future directions, including temporal information integration and attention mechanisms, provide a roadmap for continued development and refinement of this promising approach.